{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de8d3b86a4e7530583a2d5eb093b69bad13c27b4"
   },
   "source": [
    "# Machine Learning Nanodegree - Capstone project\n",
    "\n",
    "# Flower classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cb9d486fb16a3536c801211d194c8f7572b6bb8"
   },
   "source": [
    "Import necessary Python packages (for loading, plotting, and augmenting data, building Neural Network, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "8a35cfee1862b9f88610f11ae19c4fe17a895c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=theano\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=theano\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "from keras import backend as k\n",
    "from keras import optimizers, regularizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, GlobalAveragePooling2D, Input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa2f43882007c3fc703c28eab32f59dbf810cf8f"
   },
   "source": [
    "Splitting into three folders: Train, validation, Test. And setup hyper-parameters for tuning later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "f170af890f5bd8bba23780289290053ffba2acb1"
   },
   "outputs": [],
   "source": [
    "test_labels_dir = '../files/imagelabels.mat'\n",
    "benchmark_model_dir = '../models/benchmark_model.h5'\n",
    "best_model_dir = '../models/best_model.h5'\n",
    "final_model_dir = '../models/final_model.h5'\n",
    "data_dir = '../flower_data'\n",
    "\n",
    "# test_labels_dir = '../input/test-file/imagelabels.mat'\n",
    "# benchmark_model_dir = 'benchmark_model.h5'\n",
    "# best_model_dir = 'best_model.h5'\n",
    "# final_model_dir = 'final_model.h5'\n",
    "# data_dir = '../input/flower-data-2/flower_data/flower_data'\n",
    "\n",
    "cat_to_name_dir = 'cat_to_name.json'\n",
    "TRAIN, VAL, TEST = 'train', 'valid', 'test'\n",
    "\n",
    "train_in_file = os.path.join(data_dir, TRAIN)\n",
    "valid_in_file = os.path.join(data_dir, VAL)\n",
    "test_in_file = os.path.join(data_dir, TEST)\n",
    "categories_in_file = os.path.join(data_dir, cat_to_name_dir)\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = (224, 224) # This size is dertermined by the size from VGG-19\n",
    "channels = 3 # RGB\n",
    "epochs = 30\n",
    "transform_ratio = 0.2 # For Data augmentation aggressiveness \n",
    "batch_size = 64\n",
    "rescale_ratio = 1.0 / 255.0 # (RGB)\n",
    "num_unfrozen_layers = 6 # for freezing layers later during Transfer Learning (we will create 6 custom layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "f170af890f5bd8bba23780289290053ffba2acb1"
   },
   "outputs": [],
   "source": [
    "# Save checkpoint in a target directory with accuracy\n",
    "def get_checkpoint(target_directory):\n",
    "     return ModelCheckpoint(\n",
    "        target_directory,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    mode='auto',\n",
    "    min_delta=0.0, # Tuning this parameter for smallest amount to be `improvement`\n",
    "    patience=5, # number of epochs with no improvement after which we stop\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "02309e9eecd2f5db76c16117b9b4d4f91c55d2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({77: 251, 73: 194, 89: 183, 81: 166, 88: 152, 46: 54})\n"
     ]
    }
   ],
   "source": [
    "test_labels = loadmat(test_labels_dir)['labels'][0][:1000]\n",
    "print(Counter(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "082a77bed1ee18f3cad445561977aae215cc973e"
   },
   "source": [
    "# Data augmentation\n",
    "\n",
    "Create Image generators for our flower data structure to have more options for our training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "d5466f3aab235f89462e99a4309ca71c4d27e7f3"
   },
   "outputs": [],
   "source": [
    "## Augmentation configuration is applied to Image Data Generator for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    horizontal_flip=True,\n",
    "    rescale=rescale_ratio,\n",
    "    rotation_range=25,\n",
    "    zoom_range=transform_ratio,\n",
    "    shear_range=transform_ratio,\n",
    "    cval=transform_ratio,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    horizontal_flip=True,\n",
    "    rescale=rescale_ratio,\n",
    "    rotation_range=25,\n",
    "    zoom_range=transform_ratio,\n",
    "    shear_range=transform_ratio,\n",
    "    cval=transform_ratio,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "d5466f3aab235f89462e99a4309ca71c4d27e7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6552 images belonging to 102 classes.\n",
      "Found 818 images belonging to 102 classes.\n",
      "Found 819 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "## Create Data generators for each folders, loading from given directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_in_file,\n",
    "    target_size=input_size,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\", # Since we have more then 2 classes to predict\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=valid_in_file,\n",
    "    target_size=input_size,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_in_file,\n",
    "    target_size=input_size,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=91, # A number which divides total number of test images\n",
    "    class_mode=None, # To return only the images\n",
    "    shuffle=False # Set to False to make the images are tested in order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "d5466f3aab235f89462e99a4309ca71c4d27e7f3"
   },
   "outputs": [],
   "source": [
    "## Step sizes to fit and train our model later\n",
    "train_step_size = train_generator.n // train_generator.batch_size\n",
    "valid_step_size = valid_generator.n // valid_generator.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77c3fe658ab8ffb8466b2d3680907542953c6441"
   },
   "source": [
    "# Benchmark model\n",
    "Using a very simple custom CNN to make sure our data was loaded correctly and working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "badbdfb05fc0d3d1843567c75d06a27b8dfc5a2f"
   },
   "outputs": [],
   "source": [
    "bench_model = Sequential()\n",
    "bench_model.add(Conv2D(256, kernel_size=2, input_shape=(224, 224, 3), activation='relu'))\n",
    "bench_model.add(GlobalAveragePooling2D())\n",
    "bench_model.add(Dropout(0.5))\n",
    "bench_model.add(Dense(128, activation='relu'))\n",
    "bench_model.add(Dropout(0.2))\n",
    "bench_model.add(Dense(train_generator.num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7275baf4856912f5742648ba26c5cc1186765d18"
   },
   "outputs": [],
   "source": [
    "## Create our main model\n",
    "predicted_model = Model(inputs=bench_model.input, outputs=bench_model.output)\n",
    "predicted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96881d252d136299a9af3c8ee2ab9288c1a7fc37"
   },
   "outputs": [],
   "source": [
    "## Compile our model\n",
    "predicted_model.compile(\n",
    "    loss='categorical_crossentropy', # Since we have multi-class categories in our dataset\n",
    "    optimizer=optimizers.adam(1e-3),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "## Getting callbacks\n",
    "checkpoint = get_checkpoint(benchmark_model_dir)\n",
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "098514c29f4a725c7f96311d6b805b8d9cf360a5"
   },
   "outputs": [],
   "source": [
    "## Training model\n",
    "training_history = predicted_model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    steps_per_epoch=train_step_size,\n",
    "    validation_steps=valid_step_size,\n",
    "    epochs=epochs // 6,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ebf94b1c6dfea0fb0e544885fdc6560afcb28ed"
   },
   "source": [
    "As expected, a benchmark model built with simple architecture containing only a few Dense layers is definitely not enough to produce good result, since this model knows nothing about our current dataset. \n",
    "\n",
    "We will need a model with more knowledge on lower levels, to extract the foundation features out from the given images, using a pre-trained model such as VGG-19, which has already been trained on ImageNet through 1000 images. This way we can modify this pre-trained model using Transfer Learning technique to solve our unique problem with customized Dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "319f1916a9a09f39efccef247d6e44ae31d7e8f1"
   },
   "source": [
    "## Model creation - VGG-19 Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "411406569ec6e86658582d0a26c77c8fdc1d566e"
   },
   "source": [
    "Download VGG-19 with pre-trained weights from ImageNet. We then exclude its top layer since we want to predict our own 102 different categories of flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "9133bb8e7dc182364fd31a19f39a307e60225047",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_model = VGG19(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(input_size[0], input_size[1], channels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "bdc375dcb5c25907f39c4e5a9219c576d7598be7"
   },
   "outputs": [],
   "source": [
    "def inspect_model(model):\n",
    "    for index, layer in enumerate(base_model.layers):\n",
    "        print('Layer #{}: {} - Trainable: {}\\nWeights: {}\\n'.format(index, layer, layer.trainable, layer.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "fbf4b7a29ed032d016254fc6169cea6dc87ae21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer #0: <keras.engine.input_layer.InputLayer object at 0x106ad05f8> - Trainable: False\n",
      "Weights: []\n",
      "\n",
      "Layer #1: <keras.layers.convolutional.Conv2D object at 0x121a24898> - Trainable: True\n",
      "Weights: [block1_conv1/kernel, block1_conv1/bias]\n",
      "\n",
      "Layer #2: <keras.layers.convolutional.Conv2D object at 0x121a24b38> - Trainable: True\n",
      "Weights: [block1_conv2/kernel, block1_conv2/bias]\n",
      "\n",
      "Layer #3: <keras.layers.pooling.MaxPooling2D object at 0x11fce1630> - Trainable: True\n",
      "Weights: []\n",
      "\n",
      "Layer #4: <keras.layers.convolutional.Conv2D object at 0x11ccd6390> - Trainable: True\n",
      "Weights: [block2_conv1/kernel, block2_conv1/bias]\n",
      "\n",
      "Layer #5: <keras.layers.convolutional.Conv2D object at 0x11ccd6898> - Trainable: True\n",
      "Weights: [block2_conv2/kernel, block2_conv2/bias]\n",
      "\n",
      "Layer #6: <keras.layers.pooling.MaxPooling2D object at 0x11c6f5978> - Trainable: True\n",
      "Weights: []\n",
      "\n",
      "Layer #7: <keras.layers.convolutional.Conv2D object at 0x11c70ed30> - Trainable: True\n",
      "Weights: [block3_conv1/kernel, block3_conv1/bias]\n",
      "\n",
      "Layer #8: <keras.layers.convolutional.Conv2D object at 0x11c9e20f0> - Trainable: True\n",
      "Weights: [block3_conv2/kernel, block3_conv2/bias]\n",
      "\n",
      "Layer #9: <keras.layers.convolutional.Conv2D object at 0x11bd3bac8> - Trainable: True\n",
      "Weights: [block3_conv3/kernel, block3_conv3/bias]\n",
      "\n",
      "Layer #10: <keras.layers.convolutional.Conv2D object at 0x11bc5e0b8> - Trainable: True\n",
      "Weights: [block3_conv4/kernel, block3_conv4/bias]\n",
      "\n",
      "Layer #11: <keras.layers.pooling.MaxPooling2D object at 0x11b95b588> - Trainable: True\n",
      "Weights: []\n",
      "\n",
      "Layer #12: <keras.layers.convolutional.Conv2D object at 0x11b97e400> - Trainable: True\n",
      "Weights: [block4_conv1/kernel, block4_conv1/bias]\n",
      "\n",
      "Layer #13: <keras.layers.convolutional.Conv2D object at 0x11ba8a2b0> - Trainable: True\n",
      "Weights: [block4_conv2/kernel, block4_conv2/bias]\n",
      "\n",
      "Layer #14: <keras.layers.convolutional.Conv2D object at 0x11baa3ac8> - Trainable: True\n",
      "Weights: [block4_conv3/kernel, block4_conv3/bias]\n",
      "\n",
      "Layer #15: <keras.layers.convolutional.Conv2D object at 0x11de719b0> - Trainable: True\n",
      "Weights: [block4_conv4/kernel, block4_conv4/bias]\n",
      "\n",
      "Layer #16: <keras.layers.pooling.MaxPooling2D object at 0x11df1e390> - Trainable: True\n",
      "Weights: []\n",
      "\n",
      "Layer #17: <keras.layers.convolutional.Conv2D object at 0x1214fa7f0> - Trainable: True\n",
      "Weights: [block5_conv1/kernel, block5_conv1/bias]\n",
      "\n",
      "Layer #18: <keras.layers.convolutional.Conv2D object at 0x1214f8ba8> - Trainable: True\n",
      "Weights: [block5_conv2/kernel, block5_conv2/bias]\n",
      "\n",
      "Layer #19: <keras.layers.convolutional.Conv2D object at 0x1215ab9b0> - Trainable: True\n",
      "Weights: [block5_conv3/kernel, block5_conv3/bias]\n",
      "\n",
      "Layer #20: <keras.layers.convolutional.Conv2D object at 0x12167cb38> - Trainable: True\n",
      "Weights: [block5_conv4/kernel, block5_conv4/bias]\n",
      "\n",
      "Layer #21: <keras.layers.pooling.MaxPooling2D object at 0x121729278> - Trainable: True\n",
      "Weights: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_model(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72887398c3852ca0d574048f6f068a50dad0e1a9"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Since our data is similar to images in ImageNet database, and it\n",
    "# can be considered as small (only few thousands of images in total),\n",
    "# we can freeze the all the layers except the fully-connected (FC) layers\n",
    "# since we can expect the higher-level featuers in the pre-trained model\n",
    "# to be relevant to our data, so we only train them (the FC layers).\n",
    "############################################################################\n",
    "\n",
    "## Here we train only the top layers by freezing all the layers that are already pre-trained\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Now add some custom layers into our model\n",
    "net = Flatten(name='flatten')(base_model.output)\n",
    "net = Dense(4096, activation='relu')(net)\n",
    "net = Dropout(0.5)(net)\n",
    "net = Dense(4096, activation='relu', \n",
    "            activity_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(net)\n",
    "net = Dropout(0.5)(net)\n",
    "net = Dense(train_generator.num_classes, \n",
    "            activation='softmax')(net) # 102 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72887398c3852ca0d574048f6f068a50dad0e1a9"
   },
   "outputs": [],
   "source": [
    "# Create our main model\n",
    "predicted_model = Model(inputs=base_model.input, outputs=net)\n",
    "predicted_model.summary()\n",
    "inspect_model(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24ef8b5b580400e312a559d36bdb72b529f94e23"
   },
   "outputs": [],
   "source": [
    "# Compile our model\n",
    "predicted_model.compile(\n",
    "    loss='categorical_crossentropy', # Since we have multi-class categories in our dataset\n",
    "    optimizer=optimizers.adam(1e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "checkpoint = get_checkpoint(best_model_dir)\n",
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c07d383a7eb61ed601c556dd5010abcd9116becb"
   },
   "source": [
    "## Training model (Simple version - freezing all convolutional layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6db1cea1ed6e923182389b95dad1a39f3d4980d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_history = predicted_model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    steps_per_epoch=train_step_size,\n",
    "    validation_steps=valid_step_size,\n",
    "    epochs=int(epochs * 1/3),\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f936cfb3ae700e46675fbcccce3e44bb7cda9276"
   },
   "source": [
    "## Losses Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fbc1f83e9b45ab81d94d78c84f5565546f2c8ee"
   },
   "outputs": [],
   "source": [
    "def plot_model(trainning_history):\n",
    "    train_accuracy = training_history.history['acc']\n",
    "    val_accuracy = training_history.history['val_acc']\n",
    "    \n",
    "    train_loss = training_history.history['loss']\n",
    "    val_loss = training_history.history['val_loss']\n",
    "    \n",
    "    num_epochs = range(len(train_accuracy))\n",
    "    plt.plot(num_epochs, train_accuracy, 'g', label='Training')\n",
    "    plt.plot(num_epochs, val_accuracy, 'r', label='Validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(num_epochs, train_loss, 'b', label='Training')\n",
    "    plt.plot(num_epochs, val_loss, 'r', label='Validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09503c5c4ee4d07a762e62bb7133a6384fade3ae"
   },
   "outputs": [],
   "source": [
    "plot_model(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2cc13141334c32c42bc25ffe8e1b8d8c7b4f8060"
   },
   "source": [
    "## Loading checkpoint with best weights for our model and fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "33ebfcbd490f010a8e42bd90e483538312182373",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_model.load_weights(best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "116494306ac45b414a8dc27cab09af6aa71833f5"
   },
   "outputs": [],
   "source": [
    "## Now unfreeze another last 5 layers from Convolutional base to fine-tune our model more\n",
    "pretrained_model = predicted_model.layers\n",
    "num_unfrozen_layers = -num_unfrozen_layers - 5\n",
    "\n",
    "for layer in pretrained_model[:num_unfrozen_layers]:\n",
    "    layer.trainable = False\n",
    "for layer in pretrained_model[num_unfrozen_layers:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "inspect_model(predicted_model.layers)\n",
    "predicted_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "116494306ac45b414a8dc27cab09af6aa71833f5"
   },
   "outputs": [],
   "source": [
    "predicted_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=optimizers.adam(1e-5),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "checkpoint = get_checkpoint(final_model_dir)\n",
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "116494306ac45b414a8dc27cab09af6aa71833f5"
   },
   "outputs": [],
   "source": [
    "training_history = predicted_model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    steps_per_epoch=train_step_size,\n",
    "    validation_steps=valid_step_size,\n",
    "    epochs=int(epochs * 2/3),\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3adf547fb25dc63a6343bce6d490aa71eaa2ba11"
   },
   "outputs": [],
   "source": [
    "plot_model(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a842f2cd6f50c6f98aa628d0c57631d26360177"
   },
   "outputs": [],
   "source": [
    "model_saved_json = predicted_model.to_json()\n",
    "\n",
    "with open('../models/' + 'model.json', 'w') as json_file:\n",
    "# with open('./' + 'model.json', 'w') as json_file:\n",
    "    json_file.write(model_saved_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc30046f365591b762216fc5328fb344bdd53329"
   },
   "source": [
    "## Mapping Test label into JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6193ab21048a9b7f9bca378c0f8021cd7de0f6f"
   },
   "source": [
    "Displaying the test flower names from source as ground truth labels. We will compare these labels with our predictions produced by our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "0f40a73813e0ad6cd7f59e5b9e83722ff13090e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('frangipani', 81), ('passion flower', 77), ('water lily', 73), ('wallflower', 46), ('cyclamen', 88), ('watercress', 89)}\n"
     ]
    }
   ],
   "source": [
    "with open(categories_in_file) as json_file:\n",
    "    flower_names = json.load(json_file)\n",
    "    test_flower_names = [flower_names[str(label)] for label in test_labels]\n",
    "\n",
    "print(set([(flower_names[str(label)], label) for label in test_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e5dcf6d639bbd5ca46665b1d7355df0f14d4bfe"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34fb4f6f2eceaa7b232fa13ca8ba8d1639af2e3e"
   },
   "outputs": [],
   "source": [
    "predicted_model.load_weights(final_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c4ca9ca89525ac24184746ae25f037ee30f1bc7"
   },
   "outputs": [],
   "source": [
    "evaluation = predicted_model.evaluate_generator(valid_generator, \n",
    "                                                steps=valid_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d60f2c508514a8835e89e4be15994ad2cc7f913"
   },
   "outputs": [],
   "source": [
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c36fab216c647b8c9421d6a84f1cf8e4a7c9b7d"
   },
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "prediction = predicted_model.predict_generator(test_generator,\n",
    "                                               steps=len(test_generator.labels), verbose=1)\n",
    "\n",
    "predicted_classes = np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ba6d118c9e1edbaef393cadb90fa7bfbd090076"
   },
   "outputs": [],
   "source": [
    "labels = dict((value, key) for key, value in train_generator.class_indices.items())\n",
    "predictions = [labels[k] for k in predicted_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4eb62263cfe8263069fb3698ddaa7801ff910e9"
   },
   "outputs": [],
   "source": [
    "file_indices = test_generator.filenames\n",
    "results = pd.DataFrame({\n",
    "    \"Filename\": file_indices,\n",
    "    \"Prediction\": predictions,\n",
    "    \"Grouth truth label\": test_flower_names\n",
    "})\n",
    "results.to_csv(\"results.csv\", index=False)\n",
    "results.to_csv(\"../results/results.csv\", index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20579723080b87692f29463fab04333899dd55a0"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "545f5c9e96d8a594e9299e6b3995033645d699bb"
   },
   "source": [
    "Drawing heatmap for confusion matrix on how well our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "59b1cebcc0bab6fd5b8a5502070cabe25ed331f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({77: 251, 73: 194, 89: 183, 81: 166, 88: 152, 46: 54})\n",
      "(46, 54)\n",
      "(81, 166)\n",
      "(89, 183)\n",
      "(88, 152)\n",
      "(73, 194)\n",
      "(77, 251)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'Predicted label')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flowers = Counter(test_labels)\n",
    "flowers_names = [str(key) for key in flowers.keys()]\n",
    "\n",
    "print(flowers)\n",
    "\n",
    "while flowers:\n",
    "    print(flowers.popitem())\n",
    "\n",
    "try:\n",
    "    matrix = confusion_matrix(test_flower_names, test_flower_names)\n",
    "    df = pd.DataFrame(matrix, index=flowers_names, columns=flowers_names)\n",
    "    heatmap = sns.heatmap(df, annot=True, fmt=\"d\")\n",
    "except ValueError:\n",
    "    raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), \n",
    "                             rotation=0, ha='right', fontsize=12.0)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), \n",
    "                             rotation=45, ha='right', fontsize=12.0)\n",
    "\n",
    "plt.xlabel('Ground Truth label')\n",
    "plt.ylabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd45492fb4e8fbd4ca5beda2f738a463798b0ac6"
   },
   "outputs": [],
   "source": [
    "## Cleanup session for backend\n",
    "k.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "capstone_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
